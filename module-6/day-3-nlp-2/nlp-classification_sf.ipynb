{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install progressbar2\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape reddit page (takes a reddit .json url)\n",
    "# returns posts \n",
    "\n",
    "def scraper_bike(url):\n",
    "    headers = {'User-Agent' : 'override this bad boy!'}\n",
    "    posts = []\n",
    "    after = {}\n",
    "\n",
    "    for page in progressbar(range(40)):\n",
    "        params = {'after': after}\n",
    "        pagepull = requests.get(url=url, params=params, headers=headers)\n",
    "        page_dict = pagepull.json()\n",
    "        posts.extend(page_dict['data']['children'])\n",
    "        after = page_dict['data']['after']\n",
    "        time.sleep(.2)\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert posts to DataFrame - won't allow duplicate posts since unique id 'name' is set as index\n",
    "# Extract: name (as index) and subreddit, selftext, title (as columns)\n",
    "\n",
    "def posts_to_df(post_list):\n",
    "    i = 0\n",
    "    post_dict = {}\n",
    "    \n",
    "    for post in post_list:\n",
    "        ind = post_list[i]['data']\n",
    "        post_dict[ind['name']] = [ind['subreddit'], ind['title'], ind['selftext']]\n",
    "        i += 1\n",
    "\n",
    "    df_name = pd.DataFrame(post_dict)\n",
    "    df_name = df_name.T\n",
    "    df_name.columns = ['subreddit', 'title', 'selftext']\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes scraper function and url - outputs dataframe\n",
    "\n",
    "def scrape_to_df(scrape_func, url):\n",
    "    \n",
    "    return posts_to_df(scrape_func(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you want to scrape repeatedly over time and add to a csv\n",
    "# scrape, import csv, concat, drop duplicate, and output to csv\n",
    "# takes in scraper function, url, csv filename to import, csv filename to output\n",
    "# Outputs - Concatenated DataFrame as csv\n",
    "\n",
    "def scrape_add(scrape_func, url, import_file, export_file):\n",
    "    scrape_df = posts_to_df(scrape_func(url))\n",
    "    imported_df = pd.read_csv(import_file, index_col = 'Unnamed: 0')\n",
    "    concat_df = pd.concat([imported_df, scrape_df])\n",
    "    concat_df = concat_df[~concat_df.index.duplicated(keep='first')]\n",
    "    concat_df.to_csv(export_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:32 Time:  0:00:32\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:32 Time:  0:00:32\n"
     ]
    }
   ],
   "source": [
    "# You can also put in any 2 subreddits in as the URL and get results for those\n",
    "\n",
    "nfltest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nfl.json')\n",
    "nbatest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nba.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:33 Time:  0:00:33\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:31 Time:  0:00:31\n"
     ]
    }
   ],
   "source": [
    "politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(774, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfltest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These scrape_add functions add to already built csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/CollegeBasketball/new.json', 'NCAA_Posts_Update2.csv', 'NCAA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/AskScience/new.json', 'AskSci_Posts_Update2.csv', 'AskSci_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nba/new.json', 'NBA_Posts_Update2.csv', 'NBA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nfl/new.json', 'NFL_Posts_Update2.csv', 'NFL_Posts_Update3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "\n",
    "nfltest = nfltest.drop(columns = 'selftext')\n",
    "nbatest = nbatest.drop(columns = 'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge subreddit data\n",
    "\n",
    "train = pd.concat([nfltest, nbatest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t3_dva38v</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Official /r/NFL Sidebar contest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvfdhm</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Official Week 10 R/NFL Power Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvhdmh</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] NFL clubs were informed today that a private workout will be held for Colin Kaepernick on Saturday in Atlanta. Session will include on-field work and an interview. All clubs are invited to attend, and video of both the workout and interview will be made available to clubs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvi5xb</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] NFL has flexed the Week 12 Packers-49ers game to Sunday Night Football on NBC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvrvvy</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Falcons] YEAH, YOUNGHOE! üëâ @YoungHoeKoo has been named NFC Special Teams Player of the Week!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvhocr</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Rapaport: Cam Newton would accept trade to Chicago Bears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvnxdw</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Josh Houtz] Andy Dalton says he was benched because they \"had to think about the draft\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvjqtm</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Colin K] I‚Äôm just getting word from my representatives that the NFL league office reached out to them about a workout in Atlanta on Saturday. I‚Äôve been in shape and ready for this for 3 years, can‚Äôt wait to see the head coaches and GMs on Saturday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvc1wv</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Seahawks drop in CBS power rankings after defeating 8-0 Niners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvrgg3</th>\n",
       "      <td>nfl</td>\n",
       "      <td>The 2019 ESPN Playoff Machine is live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvgzjt</th>\n",
       "      <td>nfl</td>\n",
       "      <td>5‚Äô6‚Äù Tarik Cohen complains about towels on the top shelf at 0:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvrucj</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[NFL] The AFC and NFC Offensive Players of the week are Lamar Jackson and Dalvin Cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvhnsx</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Siciliano] Geno Smith say, \"Do the math.\" : A coin toss is more likely to land on heads in OT if it lands on tails for the opening kickoff.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvlzov</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Keysor] On Derrick Henry‚Äôs touchdown run, the Chiefs only had 10 men on the field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvhuuq</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Lombardi] Kyle Shanahan was very stern talking about the 49ers WRs today. Said point blank that passes off the hands should be caught. Then on Dante Pettis: ‚ÄúHe‚Äôs had his opportunities. The more he doesn‚Äôt take advantage of his opportunities, the less opportunities he gets.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dveqwt</th>\n",
       "      <td>nfl</td>\n",
       "      <td>In 2010, the Steelers final WR depth chart was Hines Ward, Antonio Brown, Mike Wallace, Emmanuel Sanders, Antwaan Randle El and Arnaz Battle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvkqtj</th>\n",
       "      <td>nfl</td>\n",
       "      <td>The Indianapolis Colts have won the AFC East more recently than the Buffalo Bills have. What are some other hard to believe yet true facts about the NFL?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvi5zv</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Top 5 PFF edge rusher grades: Watt, Bosa, Graham, Watt, Bosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvdlih</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Dunlap] I thought Sean McVay was reportedly going to change football forever. Keep in mind Mike Tomlin just beat him with: - a backup QB - a retooled offensive line - no running game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvbafm</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Buccaneers released former first-round pick Vernon Hargreaves, per source.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvp4ts</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Ex-Bears coach John Fox wanted different QB in draft over Mitch Trubisky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvqxs8</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Which player or staff member are you surprised is still employed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvc470</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Seahawks‚Äô WR Tyler Lockett spent the night in a Bay Area Hosptial with a lower leg injury that Seattle HC Pete Carroll said Is ‚Äúa pretty severe situation right now.‚Äù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvdwrx</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Jenkins] Jamaal Williams: \"I ain't gonna lie to you. Halfway through your question, I stopped listening.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvoj92</th>\n",
       "      <td>nfl</td>\n",
       "      <td>On this day in 2005, Jon Gruden goes for 2 and the win rather than OT to cap off a wild 36-35 Tampa Bay win against Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvrt6a</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Jamal Adams Named AFC Defensive Player of the Week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvc8a3</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Just wanted to wish Al Michaels a happy 75th birthday today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvr0ej</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[NFL Matchup on ESPN]Through Week 10 of the NFL, here are the top play-action QBs according to passer rating.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvo64h</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Madson] Seattle's drives that didn't start with a 49ers turnover: 7 punts, 3 fumbles, 2 field goals, 1 interception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dvlimi</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Silva] #Ravens OC Greg Roman deserves to be 2020‚Äôs No. 1 HC candidate: Longtime elite run-game designer, Tailors offense to fit personnel rather than forcing ‚Äúmah scheme‚Äù, Overseen Kap, Tyrod, Lamar career years, Forward-thinking hire exposed to edges analytics provide in Baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du5td5</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] DLo with the buzzer beater long three at the end of Q3 to tie the game!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dv4t4t</th>\n",
       "      <td>nba</td>\n",
       "      <td>Would you like to see fans storm the court after an upset like in college basketball?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dulb4g</th>\n",
       "      <td>nba</td>\n",
       "      <td>Are the Magic a playoff team?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dug9fz</th>\n",
       "      <td>nba</td>\n",
       "      <td>Who‚Äôs your favorite ‚ÄúBottom of the Rotation‚Äù guy that never ended up making it in the league?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du4gin</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Robert \"Timelord\" Williams with an insane block on a three, leads the break and gets a hockey assist for a Javonte Green slam!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dv01bd</th>\n",
       "      <td>nba</td>\n",
       "      <td>Who are the best NBA players you would take Luka over?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du94kt</th>\n",
       "      <td>nba</td>\n",
       "      <td>Jaylen Brown Full Highlights 2019.11.09 Celtics vs Spurs - 30 Pts, 7 Rebs, 3 Asts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du66em</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Post game thread] The Oklahoma City Thunder(4-5) Defeat the Golden State Warriors(2-8) by a score of 114-108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dug1x6</th>\n",
       "      <td>nba</td>\n",
       "      <td>GAME THREAD: Denver Nuggets (6-2) @ Minnesota Timberwolves (5-3) - (November 10, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du0daa</th>\n",
       "      <td>nba</td>\n",
       "      <td>[PelicansPR] Brandon Ingram becomes the second player in #NBA history to score over 200 points while shooting over .500 from the field and .400 from three-point range in their first eight games with a team, joining Kevin Durant, who did it with Golden State in 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dv4gxo</th>\n",
       "      <td>nba</td>\n",
       "      <td>Clippers stole Kawhi and marketing material from the Raptors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dufo4h</th>\n",
       "      <td>nba</td>\n",
       "      <td>Which rules from the Euroleague would you like NBA to implement?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dui6lu</th>\n",
       "      <td>nba</td>\n",
       "      <td>GAME THREAD: Charlotte Hornets (4-5) @ Philadelphia 76ers (5-3) - (November 10, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du2fcc</th>\n",
       "      <td>nba</td>\n",
       "      <td>Paul George on Kawhi Leonard - ‚ÄúEverything is true about his makeup. He works hard, he‚Äôs a great teammate, great locker room guy. He actually does talk and he‚Äôs a fun guy. The legend is true.‚Äù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dudqan</th>\n",
       "      <td>nba</td>\n",
       "      <td>Veteran Taj Gibson Joins Kevin Knox and Mitchell Robinson for Delta Road Bites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du3gb5</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Gordon Hayward leaves the game after taking a hit from LaMarcus Aldridge on the screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_duuarb</th>\n",
       "      <td>nba</td>\n",
       "      <td>What happens if a rookie plays entire games with no rest?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dv3ys3</th>\n",
       "      <td>nba</td>\n",
       "      <td>Anyone not a fan of Kawhi Leonard?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dupepv</th>\n",
       "      <td>nba</td>\n",
       "      <td>If you are a fan of a playoff contender, would you trade your teams third best player for DeMar Derozan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dugu3r</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Velasquez] Kyle Korver is listed as out for the Bucks tonight with a head contusion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du55i9</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Hamidou Diallo hammers it down over Chriss!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du97u0</th>\n",
       "      <td>nba</td>\n",
       "      <td>Why did Kobe Bryant only take 3 shots in the 2nd half of 2006 WCQF game 7?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dukezk</th>\n",
       "      <td>nba</td>\n",
       "      <td>Stats NBA.com endpoints not working?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du69ug</th>\n",
       "      <td>nba</td>\n",
       "      <td>James Harden casually drops 42/10/9 on the Bulls.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_duc9f8</th>\n",
       "      <td>nba</td>\n",
       "      <td>Trae Young vs. De'Aaron Fox was asked a few times over the offseason. Has Trae shown enough for your opinion to change any?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du6rrk</th>\n",
       "      <td>nba</td>\n",
       "      <td>Raptor players share memories of Kawhi ahead of LA reunion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du66po</th>\n",
       "      <td>nba</td>\n",
       "      <td>[FreeDawkins] Luka Doncic Full Highlights 2019.11.09 Mavs vs Grizzlies - 24 Pts, 14 Rebs, 8 Asts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du3uwt</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Robert \"Timelord\" Williams goes high and throws down the inbound alley oop from Smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dv1jae</th>\n",
       "      <td>nba</td>\n",
       "      <td>Can someone explain what PG has done to be regularly ranked ahead of Kyrie?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_du5uk6</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Post Game Thread] The New Orleans Pelicans (2-7) defeat the Charlotte Hornets (4-5) 115-110 behind 25 points from Brandon Ingram.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1665 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "t3_dva38v       nfl   \n",
       "t3_dvfdhm       nfl   \n",
       "t3_dvhdmh       nfl   \n",
       "t3_dvi5xb       nfl   \n",
       "t3_dvrvvy       nfl   \n",
       "t3_dvhocr       nfl   \n",
       "t3_dvnxdw       nfl   \n",
       "t3_dvjqtm       nfl   \n",
       "t3_dvc1wv       nfl   \n",
       "t3_dvrgg3       nfl   \n",
       "t3_dvgzjt       nfl   \n",
       "t3_dvrucj       nfl   \n",
       "t3_dvhnsx       nfl   \n",
       "t3_dvlzov       nfl   \n",
       "t3_dvhuuq       nfl   \n",
       "t3_dveqwt       nfl   \n",
       "t3_dvkqtj       nfl   \n",
       "t3_dvi5zv       nfl   \n",
       "t3_dvdlih       nfl   \n",
       "t3_dvbafm       nfl   \n",
       "t3_dvp4ts       nfl   \n",
       "t3_dvqxs8       nfl   \n",
       "t3_dvc470       nfl   \n",
       "t3_dvdwrx       nfl   \n",
       "t3_dvoj92       nfl   \n",
       "t3_dvrt6a       nfl   \n",
       "t3_dvc8a3       nfl   \n",
       "t3_dvr0ej       nfl   \n",
       "t3_dvo64h       nfl   \n",
       "t3_dvlimi       nfl   \n",
       "...             ...   \n",
       "t3_du5td5       nba   \n",
       "t3_dv4t4t       nba   \n",
       "t3_dulb4g       nba   \n",
       "t3_dug9fz       nba   \n",
       "t3_du4gin       nba   \n",
       "t3_dv01bd       nba   \n",
       "t3_du94kt       nba   \n",
       "t3_du66em       nba   \n",
       "t3_dug1x6       nba   \n",
       "t3_du0daa       nba   \n",
       "t3_dv4gxo       nba   \n",
       "t3_dufo4h       nba   \n",
       "t3_dui6lu       nba   \n",
       "t3_du2fcc       nba   \n",
       "t3_dudqan       nba   \n",
       "t3_du3gb5       nba   \n",
       "t3_duuarb       nba   \n",
       "t3_dv3ys3       nba   \n",
       "t3_dupepv       nba   \n",
       "t3_dugu3r       nba   \n",
       "t3_du55i9       nba   \n",
       "t3_du97u0       nba   \n",
       "t3_dukezk       nba   \n",
       "t3_du69ug       nba   \n",
       "t3_duc9f8       nba   \n",
       "t3_du6rrk       nba   \n",
       "t3_du66po       nba   \n",
       "t3_du3uwt       nba   \n",
       "t3_dv1jae       nba   \n",
       "t3_du5uk6       nba   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                  title  \n",
       "t3_dva38v                                                                                                                                                                                                                                                               Official /r/NFL Sidebar contest  \n",
       "t3_dvfdhm                                                                                                                                                                                                                                                         Official Week 10 R/NFL Power Rankings  \n",
       "t3_dvhdmh  [Schefter] NFL clubs were informed today that a private workout will be held for Colin Kaepernick on Saturday in Atlanta. Session will include on-field work and an interview. All clubs are invited to attend, and video of both the workout and interview will be made available to clubs.  \n",
       "t3_dvi5xb                                                                                                                                                                                                     [Schefter] NFL has flexed the Week 12 Packers-49ers game to Sunday Night Football on NBC.  \n",
       "t3_dvrvvy                                                                                                                                                                                                 [Falcons] YEAH, YOUNGHOE! üëâ @YoungHoeKoo has been named NFC Special Teams Player of the Week!  \n",
       "t3_dvhocr                                                                                                                                                                                                                                      Rapaport: Cam Newton would accept trade to Chicago Bears  \n",
       "t3_dvnxdw                                                                                                                                                                                                      [Josh Houtz] Andy Dalton says he was benched because they \"had to think about the draft\"  \n",
       "t3_dvjqtm                                     [Colin K] I‚Äôm just getting word from my representatives that the NFL league office reached out to them about a workout in Atlanta on Saturday. I‚Äôve been in shape and ready for this for 3 years, can‚Äôt wait to see the head coaches and GMs on Saturday.  \n",
       "t3_dvc1wv                                                                                                                                                                                                                                Seahawks drop in CBS power rankings after defeating 8-0 Niners  \n",
       "t3_dvrgg3                                                                                                                                                                                                                                                         The 2019 ESPN Playoff Machine is live  \n",
       "t3_dvgzjt                                                                                                                                                                                                                              5‚Äô6‚Äù Tarik Cohen complains about towels on the top shelf at 0:32  \n",
       "t3_dvrucj                                                                                                                                                                                                         [NFL] The AFC and NFC Offensive Players of the week are Lamar Jackson and Dalvin Cook  \n",
       "t3_dvhnsx                                                                                                                                                  [Siciliano] Geno Smith say, \"Do the math.\" : A coin toss is more likely to land on heads in OT if it lands on tails for the opening kickoff.  \n",
       "t3_dvlzov                                                                                                                                                                                                            [Keysor] On Derrick Henry‚Äôs touchdown run, the Chiefs only had 10 men on the field  \n",
       "t3_dvhuuq          [Lombardi] Kyle Shanahan was very stern talking about the 49ers WRs today. Said point blank that passes off the hands should be caught. Then on Dante Pettis: ‚ÄúHe‚Äôs had his opportunities. The more he doesn‚Äôt take advantage of his opportunities, the less opportunities he gets.\"  \n",
       "t3_dveqwt                                                                                                                                                  In 2010, the Steelers final WR depth chart was Hines Ward, Antonio Brown, Mike Wallace, Emmanuel Sanders, Antwaan Randle El and Arnaz Battle  \n",
       "t3_dvkqtj                                                                                                                                     The Indianapolis Colts have won the AFC East more recently than the Buffalo Bills have. What are some other hard to believe yet true facts about the NFL?  \n",
       "t3_dvi5zv                                                                                                                                                                                                                                  Top 5 PFF edge rusher grades: Watt, Bosa, Graham, Watt, Bosa  \n",
       "t3_dvdlih                                                                                                       [Dunlap] I thought Sean McVay was reportedly going to change football forever. Keep in mind Mike Tomlin just beat him with: - a backup QB - a retooled offensive line - no running game  \n",
       "t3_dvbafm                                                                                                                                                                                                         [Schefter] Buccaneers released former first-round pick Vernon Hargreaves, per source.  \n",
       "t3_dvp4ts                                                                                                                                                                                                                      Ex-Bears coach John Fox wanted different QB in draft over Mitch Trubisky  \n",
       "t3_dvqxs8                                                                                                                                                                                                                             Which player or staff member are you surprised is still employed?  \n",
       "t3_dvc470                                                                                                              [Schefter] Seahawks‚Äô WR Tyler Lockett spent the night in a Bay Area Hosptial with a lower leg injury that Seattle HC Pete Carroll said Is ‚Äúa pretty severe situation right now.‚Äù  \n",
       "t3_dvdwrx                                                                                                                                                                                    [Jenkins] Jamaal Williams: \"I ain't gonna lie to you. Halfway through your question, I stopped listening.\"  \n",
       "t3_dvoj92                                                                                                                                                                On this day in 2005, Jon Gruden goes for 2 and the win rather than OT to cap off a wild 36-35 Tampa Bay win against Washington  \n",
       "t3_dvrt6a                                                                                                                                                                                                                                            Jamal Adams Named AFC Defensive Player of the Week  \n",
       "t3_dvc8a3                                                                                                                                                                                                                                   Just wanted to wish Al Michaels a happy 75th birthday today  \n",
       "t3_dvr0ej                                                                                                                                                                                 [NFL Matchup on ESPN]Through Week 10 of the NFL, here are the top play-action QBs according to passer rating.  \n",
       "t3_dvo64h                                                                                                                                                                          [Madson] Seattle's drives that didn't start with a 49ers turnover: 7 punts, 3 fumbles, 2 field goals, 1 interception  \n",
       "t3_dvlimi   [Silva] #Ravens OC Greg Roman deserves to be 2020‚Äôs No. 1 HC candidate: Longtime elite run-game designer, Tailors offense to fit personnel rather than forcing ‚Äúmah scheme‚Äù, Overseen Kap, Tyrod, Lamar career years, Forward-thinking hire exposed to edges analytics provide in Baltimore  \n",
       "...                                                                                                                                                                                                                                                                                                 ...  \n",
       "t3_du5td5                                                                                                                                                                                                           [Highlight] DLo with the buzzer beater long three at the end of Q3 to tie the game!  \n",
       "t3_dv4t4t                                                                                                                                                                                                         Would you like to see fans storm the court after an upset like in college basketball?  \n",
       "t3_dulb4g                                                                                                                                                                                                                                                                 Are the Magic a playoff team?  \n",
       "t3_dug9fz                                                                                                                                                                                                 Who‚Äôs your favorite ‚ÄúBottom of the Rotation‚Äù guy that never ended up making it in the league?  \n",
       "t3_du4gin                                                                                                                                                    [Highlight] Robert \"Timelord\" Williams with an insane block on a three, leads the break and gets a hockey assist for a Javonte Green slam!  \n",
       "t3_dv01bd                                                                                                                                                                                                                                        Who are the best NBA players you would take Luka over?  \n",
       "t3_du94kt                                                                                                                                                                                                             Jaylen Brown Full Highlights 2019.11.09 Celtics vs Spurs - 30 Pts, 7 Rebs, 3 Asts  \n",
       "t3_du66em                                                                                                                                                                                 [Post game thread] The Oklahoma City Thunder(4-5) Defeat the Golden State Warriors(2-8) by a score of 114-108  \n",
       "t3_dug1x6                                                                                                                                                                                                        GAME THREAD: Denver Nuggets (6-2) @ Minnesota Timberwolves (5-3) - (November 10, 2019)  \n",
       "t3_du0daa                     [PelicansPR] Brandon Ingram becomes the second player in #NBA history to score over 200 points while shooting over .500 from the field and .400 from three-point range in their first eight games with a team, joining Kevin Durant, who did it with Golden State in 2016  \n",
       "t3_dv4gxo                                                                                                                                                                                                                                  Clippers stole Kawhi and marketing material from the Raptors  \n",
       "t3_dufo4h                                                                                                                                                                                                                              Which rules from the Euroleague would you like NBA to implement?  \n",
       "t3_dui6lu                                                                                                                                                                                                         GAME THREAD: Charlotte Hornets (4-5) @ Philadelphia 76ers (5-3) - (November 10, 2019)  \n",
       "t3_du2fcc                                                                                              Paul George on Kawhi Leonard - ‚ÄúEverything is true about his makeup. He works hard, he‚Äôs a great teammate, great locker room guy. He actually does talk and he‚Äôs a fun guy. The legend is true.‚Äù  \n",
       "t3_dudqan                                                                                                                                                                                                                Veteran Taj Gibson Joins Kevin Knox and Mitchell Robinson for Delta Road Bites  \n",
       "t3_du3gb5                                                                                                                                                                                            [Highlight] Gordon Hayward leaves the game after taking a hit from LaMarcus Aldridge on the screen  \n",
       "t3_duuarb                                                                                                                                                                                                                                     What happens if a rookie plays entire games with no rest?  \n",
       "t3_dv3ys3                                                                                                                                                                                                                                                            Anyone not a fan of Kawhi Leonard?  \n",
       "t3_dupepv                                                                                                                                                                                      If you are a fan of a playoff contender, would you trade your teams third best player for DeMar Derozan?  \n",
       "t3_dugu3r                                                                                                                                                                                                         [Velasquez] Kyle Korver is listed as out for the Bucks tonight with a head contusion.  \n",
       "t3_du55i9                                                                                                                                                                                                                                       [Highlight] Hamidou Diallo hammers it down over Chriss!  \n",
       "t3_du97u0                                                                                                                                                                                                                    Why did Kobe Bryant only take 3 shots in the 2nd half of 2006 WCQF game 7?  \n",
       "t3_dukezk                                                                                                                                                                                                                                                          Stats NBA.com endpoints not working?  \n",
       "t3_du69ug                                                                                                                                                                                                                                             James Harden casually drops 42/10/9 on the Bulls.  \n",
       "t3_duc9f8                                                                                                                                                                   Trae Young vs. De'Aaron Fox was asked a few times over the offseason. Has Trae shown enough for your opinion to change any?  \n",
       "t3_du6rrk                                                                                                                                                                                                                                    Raptor players share memories of Kawhi ahead of LA reunion  \n",
       "t3_du66po                                                                                                                                                                                              [FreeDawkins] Luka Doncic Full Highlights 2019.11.09 Mavs vs Grizzlies - 24 Pts, 14 Rebs, 8 Asts  \n",
       "t3_du3uwt                                                                                                                                                                                             [Highlight] Robert \"Timelord\" Williams goes high and throws down the inbound alley oop from Smart  \n",
       "t3_dv1jae                                                                                                                                                                                                                   Can someone explain what PG has done to be regularly ranked ahead of Kyrie?  \n",
       "t3_du5uk6                                                                                                                                                            [Post Game Thread] The New Orleans Pelicans (2-7) defeat the Charlotte Hornets (4-5) 115-110 behind 25 points from Brandon Ingram.  \n",
       "\n",
       "[1665 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize (grab only word characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'] = train['title'].map(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin list of tokenized words into single string for each row\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t3_dva38v                                                                                                                                                                                                                                                            official r nfl sidebar contest\n",
       "t3_dvfdhm                                                                                                                                                                                                                                                     official week 10 r nfl power rankings\n",
       "t3_dvhdmh    schefter nfl clubs were informed today that a private workout will be held for colin kaepernick on saturday in atlanta session will include on field work and an interview all clubs are invited to attend and video of both the workout and interview will be made available to clubs\n",
       "t3_dvi5xb                                                                                                                                                                                                    schefter nfl has flexed the week 12 packers 49ers game to sunday night football on nbc\n",
       "t3_dvrvvy                                                                                                                                                                                                     falcons yeah younghoe younghoekoo has been named nfc special teams player of the week\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['title'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split and converting series to list of strings then to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y) # balance the y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    0.535135\n",
       "nba    0.464865\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline is\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our CountVectorizer. This counts the number of appearances of all the words in our training data and\n",
    "# eliminates common english stop words. 5000 max features works well for our purposes (tested various numbers). Our\n",
    "# data is already preprocessed and tokenized manually earlier. ngram_range is 1,3, although all or nearly all our\n",
    "# features are single words\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# fit our training data and test data lists to our count_vectorizer\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "print(type(train_data_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to array\n",
    "\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1248, 5000), (417, 5000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I wanted check that the features corpus was as expected - removed print statement for readability\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '09',\n",
       " '10',\n",
       " '10 2019',\n",
       " '10 32',\n",
       " '10 blazers',\n",
       " '10 blazers vs',\n",
       " '10 chapter',\n",
       " '10 game',\n",
       " '10 games',\n",
       " '10 games season',\n",
       " '10 hawks',\n",
       " '10 hawks vs',\n",
       " '10 mnf',\n",
       " '10 nfl',\n",
       " '10 points',\n",
       " '10 straight',\n",
       " '10 sunday',\n",
       " '10 suns',\n",
       " '10 suns vs',\n",
       " '10 waived',\n",
       " '10 weeks',\n",
       " '10 year',\n",
       " '10 years',\n",
       " '100',\n",
       " '100 possessions',\n",
       " '100 yard',\n",
       " '100 yards',\n",
       " '1000',\n",
       " '101',\n",
       " '102',\n",
       " '104',\n",
       " '104 year',\n",
       " '104 year old',\n",
       " '107',\n",
       " '108',\n",
       " '108 87',\n",
       " '109',\n",
       " '10th',\n",
       " '11',\n",
       " '11 10',\n",
       " '11 10 blazers',\n",
       " '11 10 hawks',\n",
       " '11 10 suns',\n",
       " '11 11',\n",
       " '11 11 mavs',\n",
       " '11 12',\n",
       " '11 12 heat',\n",
       " '11 2019',\n",
       " '11 asts',\n",
       " '11 mavs',\n",
       " '11 mavs vs',\n",
       " '11 rebounds',\n",
       " '11 rebounds assists',\n",
       " '11 rebs',\n",
       " '11 shooting',\n",
       " '11 signed',\n",
       " '110',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '119',\n",
       " '12',\n",
       " '12 2019',\n",
       " '12 22',\n",
       " '12 heat',\n",
       " '12 heat vs',\n",
       " '12 points',\n",
       " '12 team',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '125',\n",
       " '12th',\n",
       " '13',\n",
       " '13 2019',\n",
       " '13 points',\n",
       " '131',\n",
       " '136',\n",
       " '138',\n",
       " '14',\n",
       " '15',\n",
       " '15 fga',\n",
       " '15 shooting',\n",
       " '15 years',\n",
       " '16',\n",
       " '16 games',\n",
       " '16 points',\n",
       " '17',\n",
       " '173',\n",
       " '176',\n",
       " '17th',\n",
       " '18',\n",
       " '19',\n",
       " '1950',\n",
       " '1972',\n",
       " '1972 dolphins',\n",
       " '1972 dolphins remain',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1988',\n",
       " '1990',\n",
       " '1st',\n",
       " '1st amp',\n",
       " '1st amp run',\n",
       " '20',\n",
       " '20 pts',\n",
       " '20 yards',\n",
       " '20 yards downfield',\n",
       " '200',\n",
       " '2003',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2014 cardinals',\n",
       " '2014 cardinals win',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2017 season',\n",
       " '2018',\n",
       " '2019',\n",
       " '2019 11',\n",
       " '2019 11 10',\n",
       " '2019 11 11',\n",
       " '2019 11 12',\n",
       " '2019 nfl',\n",
       " '2019 season',\n",
       " '202',\n",
       " '2020',\n",
       " '21',\n",
       " '21 days',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '24 targets',\n",
       " '24 yard',\n",
       " '25',\n",
       " '25 points',\n",
       " '25 points rebounds',\n",
       " '25 ppg',\n",
       " '250',\n",
       " '26',\n",
       " '27',\n",
       " '27 pts',\n",
       " '28',\n",
       " '28 14',\n",
       " '29',\n",
       " '2m',\n",
       " '2nd',\n",
       " '2nd amp',\n",
       " '2nd half',\n",
       " '2nd worst',\n",
       " '2nd worst shooting',\n",
       " '30',\n",
       " '30 points',\n",
       " '30 pts',\n",
       " '30 pts rebs',\n",
       " '300',\n",
       " '300 passing',\n",
       " '300 passing yards',\n",
       " '300 yard',\n",
       " '300 yard passing',\n",
       " '31',\n",
       " '31 points',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '34 pts',\n",
       " '34 pts asts',\n",
       " '35',\n",
       " '38',\n",
       " '39',\n",
       " '3p',\n",
       " '3rd',\n",
       " '3rd amp',\n",
       " '3rd quarter',\n",
       " '40',\n",
       " '400',\n",
       " '41',\n",
       " '42',\n",
       " '42 pts',\n",
       " '42 pts 11',\n",
       " '43',\n",
       " '45',\n",
       " '47',\n",
       " '48']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression model\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1248, 5000), (1248,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape check\n",
    "\n",
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983974358974359"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9424460431654677"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dataframe that matches features to coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features': vectorizer.get_feature_names(),\n",
    "                        'coefs': coef_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>nba</td>\n",
       "      <td>-2.420896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>knicks</td>\n",
       "      <td>-1.464026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>raptors</td>\n",
       "      <td>-1.372514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>lebron</td>\n",
       "      <td>-1.202379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>tonight</td>\n",
       "      <td>-1.185853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>kawhi</td>\n",
       "      <td>-1.184155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>suns</td>\n",
       "      <td>-1.009430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>players</td>\n",
       "      <td>-0.952934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>harden</td>\n",
       "      <td>-0.915225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>basketball</td>\n",
       "      <td>-0.905977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>clippers</td>\n",
       "      <td>-0.890313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>warriors</td>\n",
       "      <td>-0.888497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>trae</td>\n",
       "      <td>-0.858802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>kyrie</td>\n",
       "      <td>-0.819206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>lakers</td>\n",
       "      <td>-0.811541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>standings</td>\n",
       "      <td>-0.808428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>magic</td>\n",
       "      <td>-0.789953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>luka</td>\n",
       "      <td>-0.781961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>shooting</td>\n",
       "      <td>-0.767049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>shot</td>\n",
       "      <td>-0.748841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>boucher</td>\n",
       "      <td>-0.746681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>celtics</td>\n",
       "      <td>-0.710112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4511</th>\n",
       "      <td>steph</td>\n",
       "      <td>-0.701535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>heat</td>\n",
       "      <td>-0.698757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>tomorrow</td>\n",
       "      <td>-0.686312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>davis</td>\n",
       "      <td>-0.683984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>cavs</td>\n",
       "      <td>-0.682394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>curry</td>\n",
       "      <td>-0.673518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4898</th>\n",
       "      <td>wiggins</td>\n",
       "      <td>-0.660553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>anthony</td>\n",
       "      <td>-0.647470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>nfc</td>\n",
       "      <td>0.821469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>yates</td>\n",
       "      <td>0.826055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>falcons</td>\n",
       "      <td>0.841313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>win</td>\n",
       "      <td>0.845285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>saints</td>\n",
       "      <td>0.846461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717</th>\n",
       "      <td>touchdown</td>\n",
       "      <td>0.850413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>bengals</td>\n",
       "      <td>0.865885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>picks</td>\n",
       "      <td>0.871582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>bears</td>\n",
       "      <td>0.885602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>browns</td>\n",
       "      <td>0.886232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>wilson</td>\n",
       "      <td>0.899120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4866</th>\n",
       "      <td>week 10</td>\n",
       "      <td>0.900658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>dolphins</td>\n",
       "      <td>0.913905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>lamar</td>\n",
       "      <td>0.930278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>jackson</td>\n",
       "      <td>0.941010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>fitzpatrick</td>\n",
       "      <td>0.965868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>dak</td>\n",
       "      <td>1.018911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>qb</td>\n",
       "      <td>1.065728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>td</td>\n",
       "      <td>1.122677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4816</th>\n",
       "      <td>vikings</td>\n",
       "      <td>1.169207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>chiefs</td>\n",
       "      <td>1.170202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>ravens</td>\n",
       "      <td>1.187305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>steelers</td>\n",
       "      <td>1.202918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>rams</td>\n",
       "      <td>1.205635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>giants</td>\n",
       "      <td>1.208808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>week</td>\n",
       "      <td>1.327025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>yards</td>\n",
       "      <td>1.329490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>49ers</td>\n",
       "      <td>1.358553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>seahawks</td>\n",
       "      <td>1.358590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>nfl</td>\n",
       "      <td>2.389848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         features     coefs\n",
       "1878          nba -2.420896\n",
       "1489       knicks -1.464026\n",
       "3252      raptors -1.372514\n",
       "1544       lebron -1.202379\n",
       "4704      tonight -1.185853\n",
       "1453        kawhi -1.184155\n",
       "4543         suns -1.009430\n",
       "2125      players -0.952934\n",
       "1206       harden -0.915225\n",
       "400    basketball -0.905977\n",
       "630      clippers -0.890313\n",
       "4851     warriors -0.888497\n",
       "4729         trae -0.858802\n",
       "1506        kyrie -0.819206\n",
       "1511       lakers -0.811541\n",
       "4486    standings -0.808428\n",
       "1648        magic -0.789953\n",
       "1639         luka -0.781961\n",
       "4413     shooting -0.767049\n",
       "4417         shot -0.748841\n",
       "462       boucher -0.746681\n",
       "568       celtics -0.710112\n",
       "4511        steph -0.701535\n",
       "1227         heat -0.698757\n",
       "4702     tomorrow -0.686312\n",
       "752         davis -0.683984\n",
       "565          cavs -0.682394\n",
       "714         curry -0.673518\n",
       "4898      wiggins -0.660553\n",
       "331       anthony -0.647470\n",
       "...           ...       ...\n",
       "1906          nfc  0.821469\n",
       "4971        yates  0.826055\n",
       "956       falcons  0.841313\n",
       "4905          win  0.845285\n",
       "4315       saints  0.846461\n",
       "4717    touchdown  0.850413\n",
       "420       bengals  0.865885\n",
       "2096        picks  0.871582\n",
       "403         bears  0.885602\n",
       "496        browns  0.886232\n",
       "4904       wilson  0.899120\n",
       "4866      week 10  0.900658\n",
       "831      dolphins  0.913905\n",
       "1512        lamar  0.930278\n",
       "1372      jackson  0.941010\n",
       "1012  fitzpatrick  0.965868\n",
       "719           dak  1.018911\n",
       "2845           qb  1.065728\n",
       "4578           td  1.122677\n",
       "4816      vikings  1.169207\n",
       "600        chiefs  1.170202\n",
       "3345       ravens  1.187305\n",
       "4507     steelers  1.202918\n",
       "3082         rams  1.205635\n",
       "1123       giants  1.208808\n",
       "4865         week  1.327025\n",
       "4952        yards  1.329490\n",
       "201         49ers  1.358553\n",
       "4353     seahawks  1.358590\n",
       "1908          nfl  2.389848\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's throw out these unfair words and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "extra_stopwords = ['nba', 'basketball', 'football', 'nfl']\n",
    "\n",
    "stopwords.update(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1248, 5000), (417, 5000))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stopwords,\n",
    "                             max_features = 5000,\n",
    "                             ngram_range = (1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975961538461539"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9112709832134293"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>knicks</td>\n",
       "      <td>-1.438674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>raptors</td>\n",
       "      <td>-1.375928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>kawhi</td>\n",
       "      <td>-1.189662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>lebron</td>\n",
       "      <td>-1.098686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>tonight</td>\n",
       "      <td>-1.063112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>suns</td>\n",
       "      <td>-1.049710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>kyrie</td>\n",
       "      <td>-0.950605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>harden</td>\n",
       "      <td>-0.923377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>clippers</td>\n",
       "      <td>-0.883210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>trae</td>\n",
       "      <td>-0.867771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>standings</td>\n",
       "      <td>-0.864051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>warriors</td>\n",
       "      <td>-0.853749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>lakers</td>\n",
       "      <td>-0.828720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>luka</td>\n",
       "      <td>-0.823976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>magic</td>\n",
       "      <td>-0.817989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>shooting</td>\n",
       "      <td>-0.784612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>players</td>\n",
       "      <td>-0.736643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>celtics</td>\n",
       "      <td>-0.729482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>shot</td>\n",
       "      <td>-0.722068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>stephen</td>\n",
       "      <td>-0.706691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>november</td>\n",
       "      <td>-0.703845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>heat</td>\n",
       "      <td>-0.702421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>best</td>\n",
       "      <td>-0.693505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>boucher</td>\n",
       "      <td>-0.684845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>tomorrow</td>\n",
       "      <td>-0.675426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>wiggins</td>\n",
       "      <td>-0.660409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>steph</td>\n",
       "      <td>-0.645369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>three</td>\n",
       "      <td>-0.634177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>star</td>\n",
       "      <td>-0.632258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>slam</td>\n",
       "      <td>-0.628598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>yard</td>\n",
       "      <td>0.781380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>nfc</td>\n",
       "      <td>0.805480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>yates</td>\n",
       "      <td>0.822349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>bengals</td>\n",
       "      <td>0.839317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>picks</td>\n",
       "      <td>0.848515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>touchdown</td>\n",
       "      <td>0.857702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>saints</td>\n",
       "      <td>0.886956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>kyler</td>\n",
       "      <td>0.887276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>lamar</td>\n",
       "      <td>0.910310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>browns</td>\n",
       "      <td>0.912643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>dolphins</td>\n",
       "      <td>0.913130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>wilson</td>\n",
       "      <td>0.934621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>jackson</td>\n",
       "      <td>0.936230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>falcons</td>\n",
       "      <td>0.947559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>bears</td>\n",
       "      <td>0.972684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>fitzpatrick</td>\n",
       "      <td>0.995626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>week 10</td>\n",
       "      <td>0.996182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>dak</td>\n",
       "      <td>1.018152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>qb</td>\n",
       "      <td>1.114518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>td</td>\n",
       "      <td>1.146347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>rams</td>\n",
       "      <td>1.159799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>giants</td>\n",
       "      <td>1.182517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>ravens</td>\n",
       "      <td>1.199953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>chiefs</td>\n",
       "      <td>1.211474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>steelers</td>\n",
       "      <td>1.214372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>vikings</td>\n",
       "      <td>1.284535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>seahawks</td>\n",
       "      <td>1.312664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>week</td>\n",
       "      <td>1.413514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>yards</td>\n",
       "      <td>1.421853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>49ers</td>\n",
       "      <td>1.442087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         features     coefs\n",
       "1560       knicks -1.438674\n",
       "3399      raptors -1.375928\n",
       "1523        kawhi -1.189662\n",
       "1633       lebron -1.098686\n",
       "4684      tonight -1.063112\n",
       "4510         suns -1.049710\n",
       "1577        kyrie -0.950605\n",
       "1278       harden -0.923377\n",
       "651      clippers -0.883210\n",
       "4713         trae -0.867771\n",
       "4450    standings -0.864051\n",
       "4839     warriors -0.853749\n",
       "1582       lakers -0.828720\n",
       "1726         luka -0.823976\n",
       "1736        magic -0.817989\n",
       "4363     shooting -0.784612\n",
       "2704      players -0.736643\n",
       "589       celtics -0.729482\n",
       "4367         shot -0.722068\n",
       "4477      stephen -0.706691\n",
       "2497     november -0.703845\n",
       "1296         heat -0.702421\n",
       "439          best -0.693505\n",
       "479       boucher -0.684845\n",
       "4682     tomorrow -0.675426\n",
       "4887      wiggins -0.660409\n",
       "4475        steph -0.645369\n",
       "4635        three -0.634177\n",
       "4454         star -0.632258\n",
       "4406         slam -0.628598\n",
       "...           ...       ...\n",
       "4945         yard  0.781380\n",
       "2475          nfc  0.805480\n",
       "4969        yates  0.822349\n",
       "435       bengals  0.839317\n",
       "2678        picks  0.848515\n",
       "4700    touchdown  0.857702\n",
       "4259       saints  0.886956\n",
       "1575        kyler  0.887276\n",
       "1583        lamar  0.910310\n",
       "513        browns  0.912643\n",
       "855      dolphins  0.913130\n",
       "4894       wilson  0.934621\n",
       "1443      jackson  0.936230\n",
       "994       falcons  0.947559\n",
       "414         bears  0.972684\n",
       "1071  fitzpatrick  0.995626\n",
       "4853      week 10  0.996182\n",
       "740           dak  1.018152\n",
       "2993           qb  1.114518\n",
       "4547           td  1.146347\n",
       "3229         rams  1.159799\n",
       "1190       giants  1.182517\n",
       "3504       ravens  1.199953\n",
       "621        chiefs  1.211474\n",
       "4471     steelers  1.214372\n",
       "4804      vikings  1.284535\n",
       "4298     seahawks  1.312664\n",
       "4852         week  1.413514\n",
       "4951        yards  1.421853\n",
       "200         49ers  1.442087\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "coef_list = coef_list[0]\n",
    "\n",
    "coef_df = pd.DataFrame({'features' : vectorizer.get_feature_names(),\n",
    "                       'coefs' : coef_list})\n",
    "\n",
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Matrix on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking where our model failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({'y_actual' : y_test,\n",
    "             'y_predicted' : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df = comparison_df[comparison_df['y_actual'] != comparison_df['y_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch2_df = pd.concat([mismatch_df, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All incorrect predictions with titles\n",
    "\n",
    "mismatches = mismatch2_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency / Inverse Document Frequency\n",
    "\n",
    "TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(w) = log_e(Total number of documents / Number of documents with term w in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                            tokenizer=None,\n",
    "                            preprocessor=None,\n",
    "                            stop_words=['nba', 'nfl', 'football', 'basketball'],\n",
    "                            max_features=5000,\n",
    "                            ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = tfidf_vec.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = tfidf_vec.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try on some other subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([politics_test, conservative_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "# conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_test = politics_test.drop(columns='selftext')\n",
    "conservative_test = conservative_test.drop(columns='selftext')\n",
    "\n",
    "train = pd.concat([politics_test, conservative_test])\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)\n",
    "    \n",
    "    \n",
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "\n",
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features' : vectorizer.get_feature_names(),\n",
    "                       'coefs' : coef_list})\n",
    "\n",
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... for, like, actual poets. By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "\n",
    "## Why word vectors?\n",
    "\n",
    "Poetry is, at its core, the art of identifying and manipulating linguistic similarity. I have discovered a truly marvelous proof of this, which this notebook is too narrow to contain. (By which I mean: I will elaborate on this some other time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animal similarity and simple linear algebra\n",
    "\n",
    "We'll begin by considering a small subset of English: words for animals. Our task is to be able to write computer programs to find similarities among these words and the creatures they designate. To do this, we might start by making a spreadsheet of some animals and their characteristics. For example:\n",
    "\n",
    "![Animal spreadsheet](http://static.decontextualize.com/snaps/animal-spreadsheet.png)\n",
    "\n",
    "This spreadsheet associates a handful of animals with two numbers: their cuteness and their size, both in a range from zero to one hundred. (The values themselves are simply based on my own judgment. Your taste in cuteness and evaluation of size may differ significantly from mine. As with all data, these data are simply a mirror reflection of the person who collected them.)\n",
    "\n",
    "These values give us everything we need to make determinations about which animals are similar (at least, similar in the properties that we've included in the data). Try to answer the following question: Which animal is most similar to a capybara? You could go through the values one by one and do the math to make that evaluation, but visualizing the data as points in 2-dimensional space makes finding the answer very intuitive:\n",
    "\n",
    "![Animal space](http://static.decontextualize.com/snaps/animal-space.png)\n",
    "\n",
    "The plot shows us that the closest animal to the capybara is the panda bear (again, in terms of its subjective size and cuteness). One way of calculating how \"far apart\" two points are is to find their *Euclidean distance*. (This is simply the length of the line that connects the two points.) For points in two dimensions, Euclidean distance can be calculated with the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def distance2d(x1, y1, x2, y2):\n",
    "    return np.linalg.norm(np.array([x1, y1])-np.array([x2, y2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the distance between \"capybara\" (70, 30) and \"panda\" (74, 40):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance2d(70, 30, 75, 40) # panda and capybara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... is less than the distance between \"tarantula\" and \"elephant\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance2d(8, 3, 65, 90) # tarantula and elephant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling animals in this way has a few other interesting properties. For example, you can pick an arbitrary point in \"animal space\" and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken.\n",
    "\n",
    "Reasoning visually, you can also answer questions like: what's halfway between a chicken and an elephant? Simply draw a line from \"elephant\" to \"chicken,\" mark off the midpoint and find the closest animal. (According to our chart, halfway between an elephant and a chicken is a horse.)\n",
    "\n",
    "You can also ask: what's the *difference* between a hamster and a tarantula? According to our plot, it's about seventy five units of cute (and a few units of size).\n",
    "\n",
    "The relationship of \"difference\" is an interesting one, because it allows us to reason about *analogous* relationships. In the chart below, I've drawn an arrow from \"tarantula\" to \"hamster\" (in blue):\n",
    "\n",
    "![Animal analogy](http://static.decontextualize.com/snaps/animal-space-analogy.png)\n",
    "\n",
    "You can understand this arrow as being the *relationship* between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I've also transposed this same arrow (this time in red) so that its origin point is \"chicken.\" The arrow ends closest to \"kitten.\" What we've discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy:\n",
    "\n",
    "    Tarantulas are to hamsters as chickens are to kittens.\n",
    "    \n",
    "A sequence of numbers used to identify a point is called a *vector*, and the kind of math we've been doing so far is called *linear algebra.* (Linear algebra is surprisingly useful across many domains: It's the same kind of math you might do to, e.g., simulate the velocity and acceleration of a sprite in a video game.)\n",
    "\n",
    "A set of vectors that are all part of the same data set is often called a *vector space*. The vector space of animals in this section has two *dimensions*, by which I mean that each vector in the space has two numbers associated with it (i.e., two columns in the spreadsheet). The fact that this space has two dimensions just happens to make it easy to *visualize* the space by drawing a 2D plot. But most vector spaces you'll work with will have more than two dimensions‚Äîsometimes many hundreds. In those cases, it's more difficult to visualize the \"space,\" but the math works pretty much the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language with vectors: colors\n",
    "\n",
    "So far, so good. We have a system in place‚Äîalbeit highly subjective‚Äîfor talking about animals and the words used to name them. I want to talk about another vector space that has to do with language: the vector space of colors.\n",
    "\n",
    "Colors are often represented in computers as vectors with three dimensions: red, green, and blue. Just as with the animals in the previous section, we can use these vectors to answer questions like: which colors are similar? What's the most likely color name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colors, what's the name of those colors' \"average\"?\n",
    "\n",
    "We'll be working with this [color data](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json) from the [xkcd color survey](https://blog.xkcd.com/2010/05/03/color-survey-results/). The data relates a color name to the RGB value associated with that color. [Here's a page that shows what the colors look like](https://xkcd.com/color/rgb/). Download the color data and put it in the same directory as this notebook.\n",
    "\n",
    "A few notes before we proceed:\n",
    "\n",
    "* The linear algebra functions implemented below (`addv`, `meanv`, etc.) are slow, potentially inaccurate, and shouldn't be used for \"real\" code‚ÄîI wrote them so beginner programmers can understand how these kinds of functions work behind the scenes. Use [numpy](http://www.numpy.org/) for fast and accurate math in Python.\n",
    "* If you're interested in perceptually accurate color math in Python, consider using the [colormath library](http://python-colormath.readthedocs.io/en/latest/).\n",
    "\n",
    "Now, import the `json` library and load the color data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://raw.githubusercontent.com/dariusk/corpora/master/data/colors/xkcd.json')\n",
    "color_data = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts colors from hex format (`#1a2b3c`) to a tuple of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_int(s):\n",
    "    return int(s[1:3], 16), int(s[3:5], 16), int(s[5:6], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following cell creates a dictionary and populates it with mappings from color names to RGB vectors for each color in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = dict()\n",
    "for item in color_data['colors']:\n",
    "    colors[item[\"color\"]] = hex_to_int(item[\"hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 117, 0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['olive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 0, 0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 144, 0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['baby shit brown']\n",
    "co\n",
    "# #ad900d use this code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math\n",
    "\n",
    "Before we keep going, we'll need some functions for performing basic vector arithmetic. These functions will work with vectors in spaces of any number of dimensions.\n",
    "\n",
    "The first function returns the Euclidean distance between two points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def distance(coord1, coord2):\n",
    "    return np.linalg.norm(np.array(coord1)-np.array(coord2))\n",
    "distance([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subtractv` function subtracts one vector from another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, -1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtractv(coord1, coord2):\n",
    "    return list(np.array(coord1) - np.array(coord2))\n",
    "subtractv([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `addv` vector adds two vectors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 3]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addv(coord1, coord2):\n",
    "    return list(np.array(coord1) + np.array(coord2))\n",
    "addv([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `meanv` function takes a list of vectors and finds their mean or average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meanv(coords):\n",
    "    return list(np.mean(coords, axis=0))\n",
    "meanv([[0, 1], [2, 2], [4, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a test, the following cell shows that the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(colors['red'], colors['green']) > distance(colors['red'], colors['pink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the closest item\n",
    "\n",
    "Just as we wanted to find the animal that most closely matched an arbitrary point in cuteness/size space, we'll want to find the closest color name to an arbitrary point in RGB space. The easiest way to find the closest item to an arbitrary vector is simply to find the distance between the target vector and each item in the space, in turn, then sort the list from closest to farthest. The `closest()` function below does just that. By default, it returns a list of the ten closest items to the given vector.\n",
    "\n",
    "> Note: Calculating \"closest neighbors\" like this is fine for the examples in this notebook, but unmanageably slow for vector spaces of any appreciable size. As your vector space grows, you'll want to move to a faster solution, like SciPy's [kdtree](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html) or [Annoy](https://pypi.python.org/pypi/annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(space, coord, n=10):\n",
    "    return sorted(space.keys(), key=lambda x: distance(coord, space[x]))[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out, we can find the ten colors closest to \"red\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white',\n",
       " 'off white',\n",
       " 'eggshell',\n",
       " 'pale grey',\n",
       " 'ivory',\n",
       " 'cream',\n",
       " 'ecru',\n",
       " 'creme',\n",
       " 'light beige',\n",
       " 'egg shell']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, colors['white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the ten colors closest to (150, 60, 150):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['violet pink',\n",
       " 'light magenta',\n",
       " 'barbie pink',\n",
       " 'pink/purple',\n",
       " 'purple pink',\n",
       " 'candy pink',\n",
       " 'heliotrope',\n",
       " 'pink purple',\n",
       " 'purpleish pink',\n",
       " 'pinkish purple']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, [255, 60, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color magic\n",
    "\n",
    "The magical part of representing words as vectors is that the vector operations we defined earlier appear to operate on language the same way they operate on numbers. For example, if we find the word closest to the vector resulting from subtracting \"red\" from \"purple,\" we get a series of \"blue\" colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kelley green',\n",
       " 'irish green',\n",
       " 'emerald green',\n",
       " 'emerald',\n",
       " 'teal blue',\n",
       " 'teal',\n",
       " 'bluegreen',\n",
       " 'ocean',\n",
       " 'kelly green',\n",
       " 'jungle green']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, subtractv(colors['green'], colors['purple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches our intuition about RGB colors, which is that purple is a combination of red and blue. Take away the red, and blue is all you have left.\n",
    "\n",
    "You can do something similar with addition. What's blue plus green?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spearmint',\n",
       " 'aqua',\n",
       " 'wintergreen',\n",
       " 'bright turquoise',\n",
       " 'bright aqua',\n",
       " 'bright light blue',\n",
       " 'minty green',\n",
       " 'highlighter green',\n",
       " 'electric green',\n",
       " 'aqua green']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, addv(colors['blue'], colors['green']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, it's something like turquoise or cyan! What if we find the average of black and white? Predictably, we get gray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average of black and white: medium grey\n",
    "closest(colors, meanv([colors['black'], colors['white']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the tarantula/hamster example from the previous section, we can use color vectors to reason about relationships between colors. In the cell below, finding the difference between \"pink\" and \"red\" then adding it to \"blue\" seems to give us a list of colors that are to blue what pink is to red (i.e., a slightly lighter, less saturated shade):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an analogy: pink is to red as X is to blue\n",
    "pink_to_red = subtractv(colors['orange'], colors['red'])\n",
    "closest(colors, addv(pink_to_red, colors['blue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of color analogies: Navy is to blue as true green/dark grass green is to green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example: \n",
    "navy_to_blue = subtractv(colors['navy'], colors['blue'])\n",
    "closest(colors, addv(navy_to_blue, colors['green']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are fairly simple from a mathematical perspective but nevertheless *feel* magical: they're demonstrating that it's possible to use math to reason about how people use language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: A Love Poem That Loses Its Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "red = colors['red']\n",
    "blue = colors['blue']\n",
    "for i in range(14):\n",
    "    rednames = closest(colors, red)\n",
    "    bluenames = closest(colors, blue)\n",
    "    print(f\"Roses are {rednames[0]}, violets are {bluenames[0]}\")\n",
    "    red = colors[random.choice(rednames[1:])]\n",
    "    blue = colors[random.choice(bluenames[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing bad digital humanities with color vectors\n",
    "\n",
    "With the tools above in hand, we can start using our vectorized knowledge of language toward academic ends. In the following example, I'm going to calculate the average color of Bram Stoker's *Dracula*.\n",
    "\n",
    "First, we'll load [spaCy](https://spacy.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the average color, we'll follow these steps:\n",
    "\n",
    "1. Parse the text into words\n",
    "2. Check every word to see if it names a color in our vector space. If it does, add it to a list of vectors.\n",
    "3. Find the average of that list of vectors.\n",
    "4. Find the color(s) closest to that average vector.\n",
    "\n",
    "The following cell performs steps 1-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "dracula = nlp(resp.text)\n",
    "# use word.lower_ to normalize case\n",
    "drac_colors = [colors[word.lower_] for word in dracula if word.lower_ in colors]\n",
    "avg_color = meanv(drac_colors)\n",
    "print(avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll pass the averaged color vector to the `closest()` function, yielding... well, it's just a brown mush, which is kinda what you'd expect from adding a bunch of colors together willy-nilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest(colors, avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, here's what we get when we average the colors of Charlotte Perkins Gilman's classic *The Yellow Wallpaper*. The result definitely reflects the content of the story, so maybe we're on to something here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/1952/pg1952.txt')\n",
    "yellow = nlp(resp.text)\n",
    "wallpaper_colors = [colors[word.lower_] for word in yellow if word.lower_ in colors]\n",
    "avg_color = meanv(wallpaper_colors)\n",
    "closest(colors, avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: Use the vector arithmetic functions to rewrite a text, making it...\n",
    "\n",
    "* more blue (i.e., add `colors['blue']` to each occurrence of a color word); or\n",
    "* more light (i.e., add `colors['white']` to each occurrence of a color word); or\n",
    "* darker (i.e., attenuate each color. You might need to write a vector multiplication function to do this one right.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics\n",
    "\n",
    "In the previous section, the examples are interesting because of a simple fact: colors that we think of as similar are \"closer\" to each other in RGB vector space. In our color vector space, or in our animal cuteness/size space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing. They're also, for many purposes, *functionally identical*. Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it's probably also okay to show them results for, say,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cname in closest(colors, colors['mauve']):\n",
    "    print(cname, \"trousers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good for color words, which intuitively seem to exist in a multidimensional continuum of perception, and for our animal space, where we've written out the vectors ahead of time. But what about... arbitrary words? Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property?\n",
    "\n",
    "To answer that, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), which states that:\n",
    "\n",
    "    Linguistic items with similar distributions have similar meanings.\n",
    "    \n",
    "What's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n",
    "\n",
    "    It was really cold yesterday.\n",
    "    It will be really warm today, though.\n",
    "    It'll be really hot tomorrow!\n",
    "    Will it be really cool Tuesday?\n",
    "    \n",
    "According to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n",
    "\n",
    "In other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors by counting contexts\n",
    "\n",
    "So how do we turn this insight from the Distributional Hypothesis into a system for creating general-purpose vectors that capture the meaning of words? Maybe you can see where I'm going with this. What if we made a *really big* spreadsheet that had one column for every context for every word in a given source text. Let's use a small source text to begin with, such as this excerpt from Dickens:\n",
    "\n",
    "    It was the best of times, it was the worst of times.\n",
    "\n",
    "Such a spreadsheet might look something like this:\n",
    "\n",
    "![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n",
    "\n",
    "The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n",
    "\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
    "    \n",
    "Because there are ten possible contexts, this is a ten dimensional space! It might be strange to think of it, but you can do vector arithmetic on vectors with ten dimensions just as easily as you can on vectors with two or three dimensions, and you could use the same distance formula that we defined earlier to get useful information about which vectors in this space are similar to each other. In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the ___ of`):\n",
    "\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model.\n",
    "\n",
    "### Contexts and dimensionality\n",
    "\n",
    "Of course, in a corpus of any reasonable size, there will be many thousands if not many millions of possible contexts. It's difficult enough working with a vector space of ten dimensions, let alone a vector space of a million dimensions! It turns out, though, that many of the dimensions end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors. The process of getting rid of superfluous dimensions in a vector space is called [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), and most implementations of count-based word vectors make use of dimensionality reduction so that the resulting vector space has a reasonable number of dimensions (say, 100‚Äî300, depending on the corpus and application).\n",
    "\n",
    "The question of how to identify a \"context\" is itself very difficult to answer. In the toy example above, we've said that a \"context\" is just the word that precedes and the word that follows. Depending on your implementation of this procedure, though, you might want a context with a bigger \"window\" (e.g., two words before and after), or a non-contiguous window (skip a word before and after the given word). You might exclude certain \"function\" words like \"the\" and \"of\" when determining a word's context, or you might [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) the words before you begin your analysis, so two occurrences with different \"forms\" of the same word count as the same context. These are all questions open to research and debate, and different implementations of procedures for creating count-based word vectors make different decisions on this issue.\n",
    "\n",
    "### GloVe vectors\n",
    "\n",
    "But you don't have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll be using for the rest of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors in spaCy\n",
    "\n",
    "Okay, let's have some fun with real word vectors. We're going to use the GloVe vectors that come with spaCy to creatively analyze and manipulate the text of Bram Stoker's *Dracula*. First, make sure you've got `spacy` imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dracula[560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously we've used the _sm model, which doesn't include all vectors.\n",
    "# !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.0/en_core_web_lg-2.2.0.tar.gz\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "dracula = nlp(resp.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below creates a list of unique words (or tokens) in the text, as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the words in the text file\n",
    "tokens = list(set([w.text for w in dracula if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the vector of any word in spaCy's vocabulary using the `vocab` attribute, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp.vocab['alligator'].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity and finding closest neighbors\n",
    "\n",
    "The cell below defines a function `cosine()`, which returns the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of two vectors. Cosine similarity is another way of determining how similar two vectors are, which is more suited to high-dimensional spaces. [See the Encyclopedia of Distances for more information and even more ways of determining vector similarity.](http://www.uco.es/users/ma1fegan/Comunes/asignaturas/vision/Encyclopedia-of-distances-2009.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`, thereby demonstrating that the vectors are working how we expect them to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cosine_similarity(vec('dog').reshape(1, -1), vec('puppy').reshape(1, -1))[0][0] > \n",
    "    cosine_similarity(vec('trousers').reshape(1, -1), vec('octopus').reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine_similarity(vec_to_check.reshape(1, -1), vec(x))[0][0],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can get a list of synonyms, or words closest in meaning (or distribution, depending on how you look at it), to any arbitrary word in spaCy's vocabulary. In the following example, we're finding the words in *Dracula* closest to \"basketball\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what's the closest equivalent of basketball?\n",
    "spacy_closest(tokens, vec(\"basketball\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with spaCy, Dracula, and vector arithmetic\n",
    "\n",
    "Now we can start doing vector arithmetic and finding the closest words to the resulting vectors. For example, what word is closest to the halfway point between day and night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# halfway between day and night\n",
    "spacy_closest(tokens, meanv([vec(\"day\"), vec(\"night\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations of `night` and `day` are still closest, but after that we get words like `evening` and `morning`, which are indeed halfway between day and night!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the closest words in _Dracula_ to \"wine\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, vec(\"wine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you subtract \"alcohol\" from \"wine\" and find the closest words to the resulting vector, you're left with simply a lovely dinner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, subtractv(vec(\"wine\"), vec(\"alcohol\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest words to \"water\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, vec(\"water\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you add \"frozen\" to \"water,\" you get \"ice\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, addv(vec(\"water\"), vec(\"frozen\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do analogies! For example, the words most similar to \"grass\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy_closest(tokens, vec(\"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take the difference of \"blue\" and \"sky\" and add it to grass, you get the analogous word (\"green\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analogy: blue is to sky as X is to grass\n",
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens, addv(blue_to_sky, vec(\"grass\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the vector for a sentence, we simply average its component vectors, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the sentence in our text file that is closest in \"meaning\" to an arbitrary input sentence. First, we'll get the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of sentences from a spaCy parse and compares them to an input sentence, sorting them by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine_similarity(np.array(sentvec(str(x))).reshape(1, -1), np.array(input_vec).reshape(1, -1))[0][0],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the sentences in *Dracula* closest in meaning to \"My favorite food is strawberry ice cream.\" (Extra linebreaks are present because we didn't strip them out when we originally read in the source text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in spacy_closest_sent(sentences, \"My favorite food is strawberry ice cream.\"):\n",
    "    print(sent.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources\n",
    "\n",
    "* [Word2vec](https://en.wikipedia.org/wiki/Word2vec) is another procedure for producing word vectors which uses a predictive approach rather than a context-counting approach. [This paper](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf) compares and contrasts the two approaches. (Spoiler: it's kind of a wash.)\n",
    "* If you want to train your own word vectors on a particular corpus, the popular Python library [gensim](https://radimrehurek.com/gensim/) has an implementation of Word2Vec that is relatively easy to use. [There's a good tutorial here.](https://rare-technologies.com/word2vec-tutorial/)\n",
    "* When you're working with vector spaces with high dimensionality and millions of vectors, iterating through your entire space calculating cosine similarities can be a drag. I use [Annoy](https://pypi.python.org/pypi/annoy) to make these calculations faster, and you should consider using it too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
